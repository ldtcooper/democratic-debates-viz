{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599929852164",
   "display_name": "Python 3.8.5 64-bit ('dem-viz': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "debates = pd.read_csv('./raw_debates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    206.000000\nmean      15.834951\nstd       25.881756\nmin        1.000000\n25%        3.000000\n50%        5.000000\n75%       16.000000\nmax      174.000000\nName: dialog, dtype: float64"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Looking at summary stats about the size of unknown speaker dialog\n",
    "speaker_or_moderator = re.compile(r'(Speaker|Moderator|Audience)')\n",
    "unknown = debates['speaker'].str.contains(speaker_or_moderator)\n",
    "unkowns_only = debates[unknown]\n",
    "words_in_unknown = unkowns_only['dialog'].str.split().str.len()\n",
    "words_in_unknown.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    6045.000000\nmean       47.757486\nstd        50.729882\nmin         1.000000\n25%         6.000000\n50%        24.000000\n75%        81.000000\nmax       304.000000\nName: dialog, dtype: float64"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# comparing to size of known speaker dialog\n",
    "debate_without_unknowns = debates[~unknown]\n",
    "words_in_known = debate_without_unknowns['dialog'].str.split().str.len()\n",
    "words_in_known.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown Cleanup\n",
    "Taking a look at the data immediatly after scraping, there was one problem that immediatly stood out: not all of the speakers could be identified. Almost all of them were, but some were marked with `\"Speaker <number>\"` or `\"Moderator <number>\"`. While this could have been fixed with a careful examination of the original videos alongside the transcrips, that would have been incredibly time-consuming for over 200 rows.\n",
    "\n",
    "Looking at summary statistics for the number of words in dialog by unknown vs. known speakers, we can see that the mean dialog (pun intended) spoken by an unknown speaker is less than one quarter the length of that of a known speaker. It is also much more closely clustered around that smaller size (with a SD of ~15.83 versus the SD of ~47.76 for known speakers). \n",
    "\n",
    "Because of the relatively small number of unknown lines (~3% of the total), and the small size of those lines, we are simply removing the rows with unknown speakers. After doing so, our next step will be to clean up the names of the remaining speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['A. Cooper', 'Abby P', 'Abby Phillip', 'Abby Phillips', 'Adam Sexton', 'Amna', 'Amna Nawaz', 'Amy Klobachar', 'Amy Klobuchar', 'Amy Langenfeld', 'Amy Walter', 'Anderson Cooper', 'Andrea Mitchell', 'Andrew Yang', 'Announcer', 'Ashley Parker', 'B. Pfannenstiel', 'Bennett', 'Bernie Sanders', 'Beto O’Rourke', 'Bill De Blasio', 'Bill Whitaker', 'Bill de Blasio', 'Brianne P', 'Brianne P.', 'Chuck Todd', 'Cory Booker', 'Crowd', 'Dana Bash', 'David', 'David Muir', 'Devin Dwyer', 'Diana', 'Don Lemon', 'Dr. Sanjay Gupta', 'E. Warren', 'Elizabeth W', 'Elizabeth W.', 'Elizabeth Warre', 'Elizabeth Warren', 'Eric Stalwell', 'Eric Swalwell', 'Erin Burnett', 'Female', 'Gayle King', 'George S', 'George S.', 'Gillibrand', 'Hallie Jackson', 'Helen', 'Ilia Calderón', 'J. Hickenlooper', 'Jake Tapper', 'Jay Inslee', 'Joe Biden', 'John Delaney', 'John H', 'John H.', 'John Hickenloop', 'John King', 'Jon Ralston', 'Jorge Ramos', 'Jose', 'Jose D. B.', 'Jose D.B.', 'Judy', 'Judy Woodruff', 'Julian Castro', 'Kamala Harris', 'Kirsten G.', 'Kristen Gillibr', 'Kristen Welker', 'Lester Holt', 'Lindsey', 'Linsey Davis', 'M. Williamson', 'Major Garrett', 'Male', 'Marc Lacey', 'Margaret Brennan', 'Marianne W.', 'Marianne Willia', 'Mayor Buttigieg', 'Mayor de Blasio', 'Michael Bennet', 'Michael Bloomberg', 'Mike Bloomberg', 'Monica Hernandez', 'Ms. Williamson', 'N. Henderson', 'Norah O’Donnell', 'Pete Buttigieg', 'Rachel Maddow', 'Rachel Scott', 'Savanagh G.', 'Savannah', 'Savannah G.', 'Sec. Castro', 'Sen Klobuchar', 'Senator Bennet', 'Senator Booker', 'Senator Warren', 'Stephanie Sy', 'Steve Bullock', 'Steve Kornacki', 'Tim Alberta', 'Tim Ryan', 'Tom Steyer', 'Tulsi Gabbard', 'Vanessa Hauc', 'Voiceover', 'Wolf Blitzer', 'Yamiche', 'Yamiche A.', 'Yang']\n"
    }
   ],
   "source": [
    "\n",
    "# remove unknown speakers\n",
    "debates = debate_without_unknowns\n",
    "# see all names\n",
    "unique_names = sorted(debates['speaker'].unique())\n",
    "print(unique_names)\n",
    "\n",
    "text_file = open('names.csv', 'w')\n",
    "n = text_file.write(',\\n'.join(unique_names))\n",
    "text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Names\n",
    "My next problem was the large variation in names. The same candidate could be referred to by a lot of different names. Some of these were as simple as extra spacing on the end (which I fixed in the data scraper), while others were more complicated.\n",
    "\n",
    "For example: Elizabeth Warren was in the transcripts under 6 different variations of her name, e.g. \"Elizabeth Warren,\", \"E. Warren\", \"Senator Warren\"\n",
    "\n",
    "It would have been cool to do this programatically. One solution I saw to a similar problem was to use k-mean clustering by Levenshtein Distance, but there were two problems with this approach. One, I would need to know how many clusters (i.e. people) I needed to find, and two, I'd need to take the time to implement that rather heavy way of doing things, in which time I could have done the one-time task several times over. With that in mind, I went through the unique list of names by hand and created a table that would tell me what actual names correspond to names in the transcripts. \n",
    "\n",
    "I also used that opportunity to denote which people were candidates, and which invalid entries remained after the validation in the scraping step, e.g. \"Crowd\", \"Male\", etc.\n",
    "\n",
    "You can see the results of this process in `names_conversion.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dicts for name conversion\n",
    "name_conv = pd.read_csv('./names_conversion.csv')\n",
    "conv_dict = name_conv.set_index('present_name').T.to_dict()\n",
    "\n",
    "name_dict = {}\n",
    "candidate_dict = {}\n",
    "invalid_entries = []\n",
    "# split csv into dicts and lists to clean different values\n",
    "for k in conv_dict:\n",
    "    v = conv_dict[k]\n",
    "    # invalid values are nan (float), but valid ones are strings \n",
    "    if not isinstance(v['to_name'], str):\n",
    "        invalid_entries.append(k)\n",
    "        continue\n",
    "\n",
    "    # turn non-standard names into standard names\n",
    "    name_dict[k] = v['to_name']\n",
    "\n",
    "    # determine if a speaker is a moderator\n",
    "    if v['is_candidate'] == 'y':\n",
    "        candidate_dict[v['to_name']] = True\n",
    "    else:\n",
    "        candidate_dict[v['to_name']] = False\n",
    "\n",
    "# remove all invalid entries\n",
    "invalid_entries = debates['speaker'].isin(invalid_entries)\n",
    "debates = debates[~invalid_entries]\n",
    "\n",
    "# standardize names\n",
    "debates['speaker'] = debates['speaker'].map(name_dict)\n",
    "# add candidate column\n",
    "debates['is_candidate'] = debates['speaker'].map(candidate_dict)\n",
    "\n",
    "print(debates.head(40))\n"
   ]
  },
  {
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we've taken the following steps to ensure that the data is as clean as possible:\n",
    "\n",
    "- Removed initial invalid entries (i.e. entries with unknown speakers)\n",
    "- Normalized names so that one speaker is always labelled with the same name (e.g. Michael Bloomberg is always \"Michael Bloomberg\" and never \"Mike Bloomberg\")\n",
    "- Removed more unknown speakers who weren't uncovered in the initial steps\n",
    "- Added information on who is a candidate and who isn't so that we can filter on that information later"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  }
 ]
}